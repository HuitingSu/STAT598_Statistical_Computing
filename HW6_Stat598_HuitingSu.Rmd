---
title: "LASSO Implementation"
author: "Huiting Su"
date: "April 11, 2018"
collaborator: "Stephanie Oprescu"
output: html_document
---

## Problem 1: LASSO

### (a) Function to generate training dataset.
```{r}
#n: number of observations
#p: dimensionality
gen_data <- function(n, p, sparsity, level){
    set.seed(44347)
    X = matrix(rnorm(n*p), n, p)
    w = c(rep(level, sparsity), rep(0,p - sparsity))
    Y = X%*%w + rnorm(n)
    training = list(X, Y, w)
    return(training)
}
```

### (b) Define loss function of LASSO.
```{r}
lasso_loss <- function(w, lambda){
    loss <- sum((y - X%*%w)^2) + lambda * sum(abs(w))    
    return(loss)
}
```

### (c) Generate dataset.
```{r}
training <- gen_data(50, 100, 5, 5)
X = training[[1]]
y = training[[2]]
w_true = training[[3]] 
```

### (d) Use optim to find w with lambda=1.
```{r}
opt1 <- optim(par = rep(0,100), lasso_loss, method= 'BFGS', lambda = 1)
opt1$par
```
```{r}
plot(w_true, ylim = c(-5,5), main = "Solved w vs true w")
points(opt1$par, col = 'grey')
```



### (e) Use optim to find best w and lambda.
```{r}
f_wrapper <- function(lambda_w){
    return(lasso_loss(lambda_w[-1], abs(lambda_w[1])))    
}
    
opt2 <- optim(par = c(1, rep(0,100)), f_wrapper, method= 'BFGS')
opt2$par
```

As lambda is nonnegative, I set it to be absolute value in the loss function. When optimizing at the same time, lambda is solved to be approximately 0. This is a trivial result, because when minimizing error on lambda, the penalty will also be set to 0. That is why we should not optimize lambda together with w, but use cross validation to find the optimal value of lambda.   

```{r}
plot(w_true, ylim = c(-5,5), main="Solved w vs true w")
points(opt2$par[-1], col = 'grey')
```

## Problem 2: Coordinate Descent
### 1. Calculate soft threshold.
```{r}
soft_threshold <- function(w, th){
    if(abs(w) < th){
        return(0)
    }else{
        return(sign(w)*(abs(w)-th))
    }
}

w = -5:5
plot(sapply(w, soft_threshold, th=1), type='l', col='red')
```

### 2. Solve 1-d case.
```{r}
lasso1d <- function(x1, y1, lambda){
    w = t(y1)%*%x1 / t(x1)%*%x1 
    return( soft_threshold(w, lambda/t(x1)%*%x1) )
}
lasso1d(X[,1], y, 1)
```

### 3. Get residual for some dimension.
```{r}
get_residual <- function(w, dim){
    w[dim] <- 0
    Y_pred <- X %*% w
    return(y - Y_pred)
}
```

### 4.Solve w by coordinate descent.
```{r}
coor_descent <-  function(w0, epsilon, lambda){
    w = w0
    w_prev = w0 + epsilon + 1
    while(sum(abs(w-w_prev) > epsilon)) {
        w_prev = w
        for(d in 1:length(w0)){
            rd = get_residual(w, d)
            xd = X[,d]
            w_OLS = t(xd) %*% rd / t(xd) %*% xd
            w[d] = soft_threshold(w_OLS, lambda/t(xd)%*%xd)
        } 
    }
    return(w)
}
```

### 5. Run coordinate descent on earlier dataset, with lambda = 1.
```{r}
w0 = rep(1, 100)
w_coor = coor_descent(w0, epsilon = 0.001, lambda = 1)
w_coor
```
Result looks much better than the result of optim.

### 6. 
```{r, cache = TRUE}
L2 = numeric(51)
L2[1] = sum((w_coor - w_true)^2)
for(i in 50:3){
    X = X[-i,]
    y = y[-i]
    w = coor_descent(w0, epsilon = 0.001, lambda = 1)
    L2[i + 1] = sum((w - w_true)^2)
}
# With only one sample point
X = matrix(X[-2,], 1, 100)
y = y[-2]
w = coor_descent(w0, epsilon = 0.001, lambda = 1)
L2[2] = sum((w - w_true)^2)
# 0 sample point
L2[1] = sum((w0 - w_true)^2)
```




